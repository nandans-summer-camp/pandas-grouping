{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Declarative vs Imperative Programming\n",
    "\n",
    "Declarative code declares \"what\" it wants (think: demands made by a child).\n",
    "\n",
    "Imperative code declares \"how\" it wants to do something (think: step by step instructions to bake a cake)\n",
    "\n",
    "All systems require, at some level, imperative code. The computer needs to be told exactly how to do things.\n",
    "\n",
    "There are many advantages, however, in having the primary core set of instructions for a given piece of software written in a declarative fashion. In the context of data analysis, declarative code allows us to abstract away, and therefore swap out, the way in which we actually compute the results.\n",
    "\n",
    "For example, this allows the computational engine to be different depending on the size of data and size of the computer (or number of computers!). Let's take a look at the `map` operation. In basic Python, we could write the map operation in a very imperative fashion:\n",
    "\n",
    "```python\n",
    "out = []\n",
    "for x in data:\n",
    "    out.append(fn(x))\n",
    "```\n",
    "\n",
    "When we write the `for` loop, Python (and any reader!) doesn't know what we are planning to do inside the for block. It's a generic for loop, therefore the program must do what it says: loop through the data one at a time, starting with the first piece and moving to the last, sequentially. \n",
    "\n",
    "Now let's consider the `map` in Pandas: \n",
    "\n",
    "```python\n",
    "df.map(fn)\n",
    "```\n",
    "\n",
    "Here we have declared that we want each element to be changed to the transformation implied by a given function, `fn`. We don't care how it's done! The transformation could be implemented sequentially, one at a time, but you can imagine that the computational engine could also choose to implement it in parallel, performing multiple operations at once. Similarly, this could be implimented in a distributed fashion, on multiple machines at once! \n",
    "\n",
    "We'll learn more about parallelism and distributed computing later. The takeaway here is that by using higher-level abstractions, brought to us by pandas' declarative interface, we don't have to worry about the computational process, which can be optimized for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## The split-apply-combine pattern with DataFrames\n",
    "\n",
    "Often we want to do a special operation with our data: \n",
    "\n",
    "1. Group rows together by some categorical column value (`.groupby`)\n",
    "2. Apply a function to each group, that either **maps** rows or **reduces** into a single row or value (`.apply`)\n",
    "3. Combine the results from each group into a either a Series or DataFrame. \n",
    "\n",
    "This is referred to as the **split-apply-combine** pattern and is very powerful! This is another \"declarative\" abstraction that Pandas gives us, allowing us to declare what we want (\"data grouped by x, then transformed by function y\"), without having to write out the process of how to do the grouping and transforming, step by step.\n",
    "\n",
    "Let's explore it step-by-step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "scoreboard = [{ 'player': 'kewld00d1', 'best_score': 100, 'last_score': 100, 'country': 'gr', 'level': 'n00b'},\n",
    "              { 'player': 'saphyre', 'best_score': 250, 'last_score': 120, 'country': 'gr', 'level': 'n00b'},\n",
    "              { 'player': 'chckn0rris', 'best_score': 300, 'last_score': 200, 'country': 'gr', 'level': 'expert'},\n",
    "              { 'player': 'pumpkin', 'best_score': 550, 'last_score': 20, 'country': 'gr', 'level': 'expert'},\n",
    "              { 'player': 'tr0llhuntah', 'best_score': 200, 'last_score': 150, 'country': 'no', 'level': 'n00b'},\n",
    "              { 'player': 'nynja', 'best_score': 100, 'last_score': 100, 'country': 'no', 'level': 'expert'},\n",
    "              { 'player': 'angel90210', 'best_score': 400, 'last_score': 200, 'country': 'no', 'level': 'expert'},\n",
    "              { 'player': '111111', 'best_score': 50, 'last_score': 50, 'country': 'no', 'level': 'n00b'}]\n",
    "\n",
    "df = pd.DataFrame(scoreboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f9f9ba19850>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The .groupby method has one parameter: the column(s) with\n",
    "# which you wish to make \"groups\" of rows. It returns a new\n",
    "# data type: the DataFrameGroupBy class.\n",
    "\n",
    "df.groupby('country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group: gr\n",
      "df:\n",
      "        player  best_score  last_score country   level\n",
      "0   kewld00d1         100         100      gr    n00b\n",
      "1     saphyre         250         120      gr    n00b\n",
      "2  chckn0rris         300         200      gr  expert\n",
      "3     pumpkin         550          20      gr  expert\n",
      "\n",
      "\n",
      "group: no\n",
      "df:\n",
      "         player  best_score  last_score country   level\n",
      "4  tr0llhuntah         200         150      no    n00b\n",
      "5        nynja         100         100      no  expert\n",
      "6   angel90210         400         200      no  expert\n",
      "7       111111          50          50      no    n00b\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The DataFrameGroupBy class is an iterable of 2-tuples. \n",
    "# The first element of the tuple is the \"category\" (the values\n",
    "# of the groupby columns) and the second is a mini-dataframe\n",
    "# (the rows of the original element for which the columns have the\n",
    "# values indicated by the first element of the tuple!)\n",
    "\n",
    "# Let's see it: \n",
    "\n",
    "for group, mini_df in df.groupby('country'):\n",
    "    print('group:', group)\n",
    "    print('df:\\n', mini_df)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gr: 300.0\n",
      "no: 187.5\n"
     ]
    }
   ],
   "source": [
    "# We group by because we want to apply some operation to \n",
    "# each group separately. For example, if we wanted the \n",
    "# mean of the best_score in each group: \n",
    "\n",
    "\n",
    "for group, mini_df in df.groupby('country'):\n",
    "    print(group + ':', mini_df.best_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country\n",
       "gr    300.0\n",
       "no    187.5\n",
       "dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use `.apply` to return the means as a Series object:\n",
    "\n",
    "def get_mean(df):\n",
    "    return df.best_score.mean()\n",
    "\n",
    "df.groupby('country').apply(get_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's try to filter the elements of the dataframe\n",
    "# So that we only have the best player (with the highest\n",
    "# best score) in each country.\n",
    "# .apply let's us do that too!\n",
    "\n",
    "# Steps to perform such an operation: \n",
    "\n",
    "# 1. Create a function that operates on a dataframe\n",
    "#    and returns a row with the \"best player\"\n",
    "\n",
    "def best_player(df):\n",
    "    return df\n",
    "\n",
    "# 2. Test your \"best_player\" function on the whole\n",
    "#    dataframe:\n",
    "\n",
    "best_player(df)\n",
    "\n",
    "# 3. Now group by the country and \"apply\" the function.\n",
    "#    Because your function works (on the whole df), you\n",
    "#    know it will work on the mini dataframes inside the\n",
    "#    GroupBy object\n",
    "\n",
    "# df.groupby('country').apply(best_player)\n",
    "\n",
    "# 4. Take a look at the index. `.apply` returns a new index\n",
    "#    that consists of the values used in the groupby clause.\n",
    "#    This can be very helpful when it returns a series, but\n",
    "#    if it returns a dataframe where we already have that\n",
    "#    information in a column ('country'), it doesn't help us.\n",
    "#    We can use the `.reset_index()` method to return to a \n",
    "#    numeric index. We pass the `drop=True` keyword in order\n",
    "#    to throw away the information in the current index.\n",
    "\n",
    "# df.groupby('country').apply(best_player).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Challenge:\n",
    "\n",
    "# Get the second best player by country!\n",
    "# HINT: use sort_values and iloc!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Built-in Aggregations in groupby\n",
    "\n",
    "Many aggregation functions that exist on Series and DataFrames (mean, max, min, etc.) can be called directly via the groupby object:\n",
    "\n",
    "* `.groupby(col).max()`\n",
    "* `.groupby(col).mean()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Applying aggregations directly \n",
    "# to DataFrameGroupBy object:\n",
    "\n",
    "df.groupby('country').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Challenge:\n",
    "# Get the mean score, by country AND level? \n",
    "\n",
    "def level_mean(df):\n",
    "    # Hint: you will need to group by \"level\"\n",
    "    # in this function, then get the mean best_score\n",
    "    pass\n",
    "\n",
    "df.groupby('country').apply(level_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Multiple Groupby!\n",
    "\n",
    "That groupby induction that we just performed, it's quite a common use-case! So there's an even easier way to do it in Pandas.\n",
    "\n",
    "We can group by more than one column!\n",
    "\n",
    "`.groupby([col_a, col_b])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Multiple groupby!\n",
    "# For example the task we accomplished above \n",
    "# could also be written simply as:\n",
    "\n",
    "df.groupby(['country', 'level']).mean().best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take a look at the structure of the multiple groupby!\n",
    "\n",
    "list(df.groupby(['country', 'level']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Combining DataFrames\n",
    "\n",
    "There are two primary tools for combining separate DataFrames. Not that in each case, we must think **very** carefully about what we are doing and what missing values might be created. We will look at this closely with examples.\n",
    "\n",
    "* **Concatenate:** Place one dataframe below or to the side of the other. Assuming columns or rows somehow line up via index/columns.\n",
    "* **Merge:** Combine dataframes using values in one or more columns to align rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Concatenation with `pd.concat`\n",
    "\n",
    "`pd.concat` uses the columns (when concatenating vertically) or the index (when concatenating horizontally) to line up the dataframes and put them together. \n",
    "\n",
    "What will happen if an index (column) value exists only on one of the dataframe? We can control that (with the `join` keyword parameter), but by default it will simply have a missing value in the columns (rows) that originated in the dataframe where the index (column) didn't exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Concatenation is mostly used when the rows or columns are shared. \n",
    "\n",
    "# For example, you might have data with the same columns and want \n",
    "# to concatenate them on axis 0:\n",
    "# But note: what happened to the index? \n",
    "# We might want to reset it. \n",
    "\n",
    "df1 = pd.DataFrame({\"A\": pd.Series([1,2,3]), \"B\": pd.Series([4,5,6])})\n",
    "df2 = pd.DataFrame({\"A\": pd.Series([7]), \"B\": pd.Series([10])})\n",
    "pd.concat([df1,df2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Similarly, you might have data with the same rows and different columns:\n",
    "\n",
    "df1 = pd.DataFrame({\"A\": pd.Series([1,2,3]), \"B\": pd.Series([4,5,6])})\n",
    "df2 = pd.DataFrame({\"B\": pd.Series([7,8,9]), \"C\": pd.Series([10,11,12])})\n",
    "pd.concat([df1,df2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# But note what happens if the rows do not align, and you concatenate on axis 1:\n",
    "\n",
    "df1 = pd.DataFrame({\"A\": pd.Series([1,2,3]), \"B\": pd.Series([4,5,6])})\n",
    "df2 = pd.DataFrame({\"B\": pd.Series([7]), \"C\": pd.Series([10])})\n",
    "pd.concat([df1,df2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Merge\n",
    "\n",
    "Merge is commonly used when your two dataframes must be connected and they do not share an index or columns such as when we concatenated.\n",
    "\n",
    "With merge we will connect two DataFrames on some common piece of information, e.g. a common column. The structure of the command is:\n",
    "\n",
    "```python\n",
    "pd.merge(leftdf, rightdf, how='inner', on='', *keywds)\n",
    "```\n",
    "\n",
    "* `on` defines the column(s) which should be used to line up the data. You can also define `left_on` and `right_on` separately if the columns are named differently.\n",
    "* `how` has four options: \n",
    "\n",
    "        1. 'inner': intersection of keys\n",
    "        2. 'outer': union of keys\n",
    "        3. 'left': use keys from left only\n",
    "        4. 'right': use keys from right only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Play:\n",
    "\n",
    "# Try the different merge techniques and see how the dataframes\n",
    "# combine! HINT: remember you can look at `?pd.merge` if you \n",
    "# are confused about how to use it.\n",
    "\n",
    "df1 = pd.DataFrame({\"A\": pd.Series([1,2,3]), \"B\": pd.Series([4,5,6])})\n",
    "df2 = pd.DataFrame({\"A\": pd.Series([4]), \"C\": pd.Series([7])})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Working with non-rectangular data\n",
    "\n",
    "We mentioned in the beginning that Pandas is a library for working with rectangular data.\n",
    "\n",
    "What if your data is not rectangular? What does non-rectangular data look like? Very often our data might come in dictionaries. Imagine data about a \"tweet\". It might look like this:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"screenname\": \"wunderkid\",\n",
    "    \"id_str\": \"928374987\",\n",
    "    \"text\": \"Woah, pandas is so much fun #worldrocked #jawdrop #win\",\n",
    "    \"hashtags\": [\"worldrocked\", \"jawdrop\", \"win\"]\n",
    "}\n",
    "```\n",
    "\n",
    "How would you fit this into a rectangular data format? Do the \"hashtags\" cause a problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_tweets = [{ \"screenname\": \"wunderkid\",\n",
    "          \"id_str\": \"928374987\",\n",
    "          \"text\": \"Woah, pandas is so much fun #worldrocked #jawdrop #ml\",\n",
    "          \"hashtags\": [\"worldrocked\", \"jawdrop\", \"ml\"]},\n",
    "          {\"screenname\": \"pumpkin\",\n",
    "           \"id_str\": \"98214039\",\n",
    "           \"text\": \"I stay up all night dreaming of linear models #datascience #ml\",\n",
    "           \"hashtags\": [\"datascience\", \"ml\"]}]\n",
    "\n",
    "tweets = pd.DataFrame(raw_tweets)\n",
    "\n",
    "# What is the \"hashtag\" column made of? \n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Merge keeps the data flat\n",
    "\n",
    "The correct way to use data such as this in pandas, data with nested lists, is to copy each tweet to multiple rows, one row for each hashtag.\n",
    "\n",
    "We can use \"merge\" to do this for us automatically if we put the data into two separate dataframes, one for the hashtags and one for the rest of the tweets. This is called \"normalized form\" and is often how you will find data if you get it from a SQL database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can use the \"colums\" keyword to select only some columns when\n",
    "# we create the dataframe: \n",
    "\n",
    "tweets = pd.DataFrame(raw_tweets, columns = [\"screenname\", \"id_str\", \"text\"])\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Challenge:\n",
    "# \n",
    "# Create a DataFrame object called \"hashtags\".\n",
    "# It should have two columns \"id\" and \"hashtag\".\n",
    "# \"id\" should be the id of the user who tweeted\n",
    "# the hashtag, \"hashtag\" shold be the hashtag\n",
    "# that was tweeted.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Challenge:\n",
    "\n",
    "# Try to merge the two DataFrame objects!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "name": "grouping-merging.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
